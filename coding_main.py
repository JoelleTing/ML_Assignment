# -*- coding: utf-8 -*-
"""Coding-main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m_f6k7X2Pan9cXEcLN3p-KWUuHJ3wNCM

# **Introduction**
  Football is an important part of our society. It not only provides entertainment, but also maintains the welfare of football players and generates significant revenue during FIFA and other competitions. These activities attract millions of spectators to stadiums and television sets. Numerous supporters place wagers on their preferred teams. Many of them lose their money, while a few of them succeed. We will examine the [European Soccer Database](https://www.kaggle.com/datasets/hugomathien/soccer), which is provided by Hugo Mathien, in order to predict and enhance the likelihood that the game tally will be in their favour.

  The dataset encompasses information on football players and teams from numerous European countries, as well as match results, from 2008 to 2016. The data is recorded in a SQLite database, which comprises seven tables: “Country”, “League”, “Match”, “Player”, “Player_Attributes”, “Team”, and “Team_Attributes”. The “Country” table contains 11 rows and 2 columns and has a relationship with the “League”, which has 11 rows and 3 columns, and the “Team”, which has 299 rows and 5 columns. The “Team_Attributes” table includes 1458 rows and 25 columns, each of which is related to a specific team. “Match” has 25979 rows and 115 columns that relate to the “Team” table. The “Player” table has 11060 rows and 7 columns and has a relationship with “Player_Attributes” with 183978 rows and 42 columns.

# **Literature Review**
"""

# importing the necessary libraries
# data processing, CSV file I/O (e.g. pd.read_csv)
import pandas as pd
import sqlite3
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cut_tree
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize, StandardScaler
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import squareform, pdist

from google.colab import drive
drive.mount('/content/drive/')

# connecting the database
conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/European Soccer/database.sqlite')
c = conn.cursor()

# read data into DataFrames from all the available tables
country_df = pd.read_sql("Select * from Country",conn)
league_df = pd.read_sql("Select * from League",conn)
match_df = pd.read_sql("Select * from Match",conn)
player_df = pd.read_sql("Select * from Player",conn)
player_attributes_df = pd.read_sql("Select * from Player_Attributes",conn)
team_df = pd.read_sql("Select * from Team",conn)
team_attributes_df = pd.read_sql("Select * from Team_Attributes",conn)

"""# **Data Preprocessing**

## **Data Cleaning**

### **Country and League Table**
"""

# rename the name column in 'League' table
league_df.rename(columns={'name':'league_name'},inplace=True)

"""Merging the two tables to produce a new DataFrame."""

country_league_df = country_df.merge(league_df,left_on='id',right_on='id')

country_league_df.drop(columns='country_id',inplace=True)

country_league_df.rename(columns={'id':'country_id','name':'country_name'},inplace=True)

# new DataFrame
country_league_df

"""### **Match Table**

**Missing Data**
"""

match_df.columns

# List of columns to keep
columns_to_keep = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA', 'VCH', 'VCD', 'VCA']

# Get the list of columns that have null values
columns_with_nulls = match_df.columns[match_df.isnull().any()]

# Determine which columns to drop
columns_to_drop = [col for col in columns_with_nulls if col not in columns_to_keep]

# Drop the columns with null values
match_df.drop(columns=columns_to_drop, inplace=True)

match_df.columns

match_df.dropna(axis=0, inplace=True)
#Drop all row with null values



"""It was determined that the columns denoted as "GBD", "GBA", "BSH", "BSD", "BDA", and so on, should be excluded from our study. The main rationale for this choice is the lack of information on the underlying meaning of these column names. Furthermore, the Kaggle data source does not provide any data description to elucidate their implications. Hence, in order to maintain the integrity and lucidity of our analysis, we have decided to exclude certain columns.

**Data Type Issues**
"""

# 'date' column to Datetime datatype
match_df['date'] = pd.to_datetime(match_df['date'])

"""Add two additional columns to the DataFrame with the starting and completion year of a season."""

begin_season = match_df['season'].apply(lambda x:x.split('/')[0])
finish_season = match_df['season'].apply(lambda x:x.split('/')[1])

match_df['begin_season'] = begin_season
match_df['finish_season'] = finish_season

# drop the season column which is not required anymore
# match_df.drop(columns=['season'],inplace=True,axis=1)

"""Create an additional column in the DataFrame with the year when the match was played."""

# create new column
match_df['match_year'] = match_df['date'].dt.year

"""Merge the 'Match' and 'CountryLeague' table generates a new table."""

# merge tables
match_df_v2 = match_df.merge(country_league_df,left_on='country_id',right_on='country_id')

# remove country_id as it is same as league_id are same
match_df_v2.drop(['country_id'],axis=1,inplace=True)
match_df_v2.head()

# make the two new columns as int
match_df_v2['begin_season'] = match_df_v2['begin_season'].astype('int64')
match_df_v2['finish_season'] = match_df_v2['finish_season'].astype('int64')

match_df_v2.info()

# save the cleaned match DataFrame for further reference
match_df_v2.to_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/match_merged.csv',index=False)

"""### **Player Table**

**Data Type Issues**
"""

# 'birthday' column to Datetime datatype
player_df['birthday'] = pd.to_datetime(player_df['birthday'])

# change column weight to float.
player_df.weight = player_df.weight.astype('float')

player_df.info()

"""### **Player Attributes Table**

**Missing Data**
"""

player_attributes_df.dropna(inplace=True)

player_attributes_df.isnull().sum()

"""**Contaminated Data**"""

valid_values = ['low', 'medium', 'high']

player_attributes_df = player_attributes_df[
    player_attributes_df['attacking_work_rate'].isin(valid_values) &
    player_attributes_df['defensive_work_rate'].isin(valid_values)
]

"""**Data Type Issues**"""

# change 'date' type of date column
player_attributes_df['date'] = pd.to_datetime(player_attributes_df['date'])

player_attributes_df.info()

"""Create a new table by merging the 'Player' and 'Player Attributes' tables."""

player_with_attributes_df = player_df.merge(player_attributes_df,left_on='player_api_id',right_on='player_api_id')

# rename few columns
player_with_attributes_df.rename(columns={'id_x':'id','player_fifa_api_id_x':'player_fifa_api_id'},inplace=True)

# drop the columns that have got merged twice
player_with_attributes_df.drop(['player_fifa_api_id_y','id_y'],axis=1,inplace=True)

# save the cleaned player with atrributes DataFrame for further reference
player_with_attributes_df.to_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/player_att_merged.csv',index=False)

player_with_attributes_df.info()

"""### **Team Table**"""

# drop the unwanted columns
team_df.drop(['id','team_fifa_api_id'],axis=1,inplace=True)

"""Combine the 'Team' table and 'Match' table."""

# add the home team column
match_df_v2 = match_df_v2.merge(team_df,left_on='home_team_api_id',right_on='team_api_id')
match_df_v2.rename(columns={'team_long_name':'home_team_name'},inplace=True)
match_df_v2.drop(['team_api_id','team_short_name'],axis=1,inplace=True)

# add the away team column
match_df_v2 = match_df_v2.merge(team_df,left_on='away_team_api_id',right_on='team_api_id')
match_df_v2.rename(columns={'team_long_name':'away_team_name'},inplace=True)
match_df_v2.drop(['team_api_id','team_short_name'],axis=1,inplace=True)

"""Generate a new column with the name of the victorious team for each individual match."""

# function to find and return the name of the team
def win(i):
    home_team_name = i[0]
    home_team_score = i[1]
    away_team_name = i[2]
    away_team_score = i[3]

    if home_team_score > away_team_score:
        return 'Home'
    elif home_team_score < away_team_score:
        return 'Away'
    else:
        return 'Draw'

# apply the function to the DataFrame
match_df_v2['winning_team']  = match_df_v2[['home_team_name', 'home_team_goal', 'away_team_name', 'away_team_goal']].apply(win, axis=1)

match_df_v2.head()

# update the 'Match' DataFrame
match_df_v2.to_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/match_merged.csv',index=False)

# Select the first 300 rows
match_df_first_300 = match_df_v2.head(300)

# Save these rows to a CSV file
match_df_first_300.to_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/match_merged_first_300.csv', index=False)

"""### **Team Attributes Table**

**Missing Data**
"""

team_attributes_df.drop(columns=['buildUpPlayDribbling','buildUpPlayDribblingClass'], inplace=True)

team_attributes_df.isnull().sum()

"""**Data Type Issues**"""

# change data type
team_attributes_df['date'] = pd.to_datetime(team_attributes_df['date'])

# drop unwanted columns
team_attributes_df.drop(['team_fifa_api_id'],axis=1,inplace=True)

team_attributes_df.info()

"""Create a new column to indicate the data collection date."""

team_attributes_df['year'] = team_attributes_df['date'].dt.year

"""Combine the 'Team' table with the 'Team Attributes' table."""

team_attributes_df_v2 = team_attributes_df.merge(team_df,left_on='team_api_id',right_on='team_api_id')

team_attributes_df_v2.head()

# save the cleaned team with atrributes DataFrame for further reference
team_attributes_df_v2.to_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/team_att_merged.csv',index=False)

"""# **Exploratory Data Analysis / Data Visualisation**

## **1. Which league has the most number of goals scored between 2008 and 2016?**
"""

# find the total goals scored according to league
total_league_goals = match_df_v2.groupby('league_name')[['home_team_goal','away_team_goal']].sum().sum(axis=1).sort_values\
(ascending=False)

total_league_goals

# plot the above results
sns.set()
plt.figure(figsize=(15,5))
sns.barplot(x=total_league_goals.index,y=total_league_goals)
plt.title('Highest scoring league from 2008 to 2016')
plt.xticks(rotation=35)
plt.xlabel('League')
plt.ylabel('Total Goals (2008-2016)');

"""The result informs us that LIGA BBVA of Spain is the highest scoring league with 8412 goals, followed by English Premier League with 8240 goals.

## **2. Which is the most successful team from 2008-2016?**
"""

# top 5 Teams which won the most matches
top5_teams = match_df_v2.groupby('winning_team')['winning_team'].count().sort_values(ascending=False)[1:6]
top5_teams

# plot the above results
plt.figure(figsize=(10,6))
plt.bar(top5_teams.index,top5_teams,color=['red', 'orange', 'yellow', 'green', 'blue'],edgecolor='black',width=0.5)
plt.title('Most successful team from 2008 to 2016')
plt.xlabel('Team')
plt.ylabel('Total Wins (2008-2016)');

"""FC Barcelona and Real Madrid CF are the top two continuously performing teams, who also happen to belong to LIGA BBVA league, the highest scoring league.

To my amazement, Celtic is more successful than FC Bayern and Manchester United.

## **3. Comparing the qualities of the aforesaid successful team with another team.**
"""

# filter only the numeric columns
numeric_columns = team_attributes_df_v2.select_dtypes(include='number').columns

# create a new DataFrame to compare two teams
attr_comp = pd.DataFrame(
    data=team_attributes_df_v2.query('team_long_name == "FC Barcelona"')[numeric_columns].mean(),
    columns=['Barcelona']
)

# add the column for the second team
attr_comp['Manchester United'] = team_attributes_df_v2.query('team_long_name == "Manchester United"')[numeric_columns].mean()

attr_comp

# remove the unwanted indexes
attr_comp = attr_comp[2:10]
attr_comp

# plot the above results
attr_comp.plot.barh(width=0.60,figsize=(12,8))
plt.title('Attribute Comparison of two teams')
plt.ylabel('Attribute Type')
plt.xlabel('Attribute Values')
plt.legend(loc=0);

"""Though Barcelona is more consistent than Machester United, it was just ahead in defensive qualities and was behind in chance creation and build-up play.

## **4. Better-performing teams throughout the two stages of a season.**
"""

# split the phases into 2 with 19 matches in phase 1 and the other in phase 2

phase_1 = match_df_v2.query('stage<19')
phase_2 = match_df_v2.query('stage>=19 and stage<=36')

phase_1_teams = phase_1.groupby('winning_team')['winning_team'].count().sort_values(ascending=False)[1:6]
phase_2_teams = phase_2.groupby('winning_team')['winning_team'].count().sort_values(ascending=False)[1:6]

fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(15,5))
phase_1_teams.plot(ax=axes[0],kind='bar',title='Phase 1',rot=0)
phase_2_teams.plot(ax=axes[1],kind='bar',title='Phase 2',rot=0)
axes[0].set_xlabel('Winning Team')
axes[1].set_xlabel('Winning Team')
axes[0].set_ylabel('Wins')
axes[1].set_ylabel('Wins');

"""FC Barcelona, Real Madrid CF, and Celtic continue to dominate in both halves of the season.

## **5. Which team won the English Premier League 2012-2013?**
"""

# query the important parameters
england = match_df_v2.query('country_name == "England" and begin_season == 2012 and finish_season == 2013')[['home_team_goal',\
'away_team_goal', 'home_team_name', 'away_team_name', 'winning_team']]

england.head()

# find which team won the most matches
england['winning_team'].value_counts()[0:6]

"""Manchester City still have a possibility to win the league if they remain unbeaten."""

# find if Manchester City tied any match
england.query('winning_team == "Tie" and ''(home_team_name == "Manchester City" or away_team_name == "Manchester City")')\
['winning_team'].count()

"""But as the following data reveals, Manchester City tied 9 league games, indicating that the 2012-13 English Premier League was won by Manchester United.

## **6. What teams improved the most / worst throughout these years (2008-2016)?**

The essential topic here is how to quantify the performance of a team. We believe the ideal criteria will be the win ratio, i.e., the number of victories divided by the total number of matches played every season.

And to quantify the improvement, let's compute the victory ratio for each club in the first and last seasons of our data interval period and then subtract them, i.e., last minus first, to obtain an indication of how much a team improved or even how much it deteriorated.
"""

# function that generate a new column that represents the result in terms of win, tie or loss from the perspective of the home team

def home_result_func(row):
    if row['home_team_goal'] > row['away_team_goal']:
        val = 'Win'
    elif row['home_team_goal'] == row['away_team_goal']:
        val = 'Tie'
    else:
        val = 'Loss'
    return val

match_df_v2['home_result'] = match_df_v2.apply(home_result_func, axis=1)

match_df_v2.head()

# to compute the win ratio, we need to know how many matches a team played per season
no_of_matches_played_as_home = match_df.groupby(['season','home_team_api_id'])['match_api_id'].count()

no_of_matches_played_as_home

no_of_matches_played_as_away = match_df_v2.groupby(['season','away_team_api_id'])['match_api_id'].count()

no_of_matches_played_as_away

# to compute the win ratio, we need to know how many matches a team won per season, either as home or away
no_of_win_as_home = match_df_v2.query(" home_result=='Win' ").groupby(['season','home_team_api_id'])['match_api_id'].count()

no_of_win_as_home

no_of_win_as_away = match_df_v2.query(" home_result=='Loss' ").groupby(['season','away_team_api_id'])['match_api_id'].count()

no_of_win_as_away

no_of_loss_as_home = match_df_v2.query(" home_result=='Loss' ").groupby(['season','home_team_api_id'])['match_api_id'].count()

no_of_loss_as_home

no_of_loss_as_away = match_df_v2.query(" home_result=='Win' ").groupby(['season','away_team_api_id'])['match_api_id'].count()

no_of_loss_as_away

"""An issue comes here as certain teams may never win a match as home or away or even never lose a match; this means that these teams would not show up in the previous series; thus, to remedy that, we need to add the missing teams with a matching value of zero."""

# a good reference series will be the no_of_matches_played_as_home, as it must include all the participating teams in this league at this season
# so a list of all the team ids will be created, and then it will be combined with the incomplete series and the missing teams will have a value of zero
teams_id_list = no_of_matches_played_as_home.index.to_list()
support_series = pd.Series(0, index = teams_id_list)

support_series

no_of_win_as_home = no_of_win_as_home.combine(support_series, max,fill_value=0)
no_of_win_as_away = no_of_win_as_away.combine(support_series, max,fill_value=0)
no_of_loss_as_home = no_of_loss_as_home.combine(support_series, max,fill_value=0)
no_of_loss_as_away = no_of_loss_as_away.combine(support_series, max,fill_value=0)

results_df = pd.DataFrame({'matches_as_home':no_of_matches_played_as_home,'matches_as_away':no_of_matches_played_as_away,'won_as_home':no_of_win_as_home, 'won_as_away':no_of_win_as_away,'lost_as_home':no_of_loss_as_home,'lost_as_away':no_of_loss_as_away})

results_df

results_df['total_played'] = results_df['matches_as_home'] + results_df['matches_as_away']
results_df['total_win'] = results_df['won_as_home'] + results_df['won_as_away']
results_df['total_lose'] = results_df['lost_as_home'] + results_df['lost_as_away']
results_df['total_draw'] = results_df['total_played'] - results_df['total_win'] - results_df['total_lose']

results_df.head()

results_df.index.names = ['season','team_api_id']

results_df['win_ratio_per_season'] = results_df['total_win'] / results_df['total_played']

results_df.head()

improvment_factor = results_df.groupby(['team_api_id'])['win_ratio_per_season'].last() - results_df.groupby(['team_api_id'])['win_ratio_per_season'].first()

improvment_factor

"""Now, after obtaining the improvement factor, we need to integrate it with team names for visualising charts."""

teams_vs_improvement = pd.merge(left=team_df, right=improvment_factor, on='team_api_id')

teams_vs_improvement.head(3)

teams_vs_improvement.rename(columns={'win_ratio_per_season':'improvement_factor'}, inplace=True)

teams_vs_improvement.head(3)

teams_vs_improvement.sort_values(by='improvement_factor',ascending=False, inplace=True)

teams_vs_improvement

teams_vs_improvement['improvement_factor'] = teams_vs_improvement['improvement_factor'] * 100

teams_vs_improvement

"""This table displays the top 10 teams based on the improvement factor between 2008 and 2016. This factor has been derived by subtracting the win ratio of the first season from the win ratio of the last season and then multiplied by 100 to represent percentage. For example, for the Paris Saint-Germain squad, the victory ratio in 2008 was 0.5 and in 2016 was 0.789; hence, the improvement factor is (0.789 - 0.5)*100 = 28.9% of all European teams ranked in the 4th position."""

chart = sns.barplot(x='improvement_factor', y='team_long_name', data=teams_vs_improvement[:10])
chart.set(title='Top 10 teams in terms of improvement between 2008 and 2016',xlabel="Improvement Percentage", ylabel="Team");

"""The chart displays the bottom 10 teams based on the improvement factor between 2008 and 2016, which suggests that the team's victory per ratio has declined a lot in 2016 from 2008."""

chart = sns.barplot(x='improvement_factor', y='team_long_name', data=teams_vs_improvement[-10:].sort_values(by='improvement_factor'))
chart.set(title='Worst 10 teams in terms of improvement between 2008 and 2016',xlabel="Deterioration Percentage", ylabel="Team");

"""## **7. Most favourite foot of players?**"""

# find the count of players with preferred foots
foot = player_with_attributes_df.groupby('preferred_foot')['preferred_foot'].count()
foot

# plot pie chart for the above result
plt.pie(foot,labels=['Left','Right'],autopct='%1.1f%%')
plt.title('Preferred Foot of players');

"""More than 75% favoured the right foot.

## **8. Do overall evaluations of athletes alter if they favour different foot to play?**
"""

# calculate the mean of overall ratings grouping by preferred foot.

foot_rating = player_with_attributes_df.groupby('preferred_foot')['overall_rating']
mean_foot_rating = foot_rating.mean()
mean_foot_rating

sns.barplot(x=['Left','Right'],y=mean_foot_rating)
plt.title('Mean overall rating of different footed players ')
plt.ylabel('Mean Rating')
plt.xlabel('Preferred Foot');

"""There is not any difference in overall ranking if players choose various feet to play.

## **9. Who are the finest players in Europe throughout these years (2008-2016)?**
"""

players_names = player_with_attributes_df.groupby(['player_api_id'])['player_name'].first()

players_names

"""The next set of charts lists the top 10 European players for each individual characteristic."""

for i, col in enumerate(player_with_attributes_df.columns):
    if (i<41):
        if ( type(player_with_attributes_df[col][0]) == type(player_with_attributes_df['overall_rating'][0]) ):
            plt.figure(i);
            mean_of_attribute = player_with_attributes_df.groupby(['player_api_id'])[col].mean()
            player_vs_att_df = pd.DataFrame({'name':players_names, col:mean_of_attribute})
            player_vs_att_df = player_vs_att_df.sort_values(by=col,ascending=False)
            chart = sns.barplot(x=col, y='name', data=player_vs_att_df[:10]);
            chart.set(ylabel='Player')

"""## **10. How are player traits linked to one another?**

**Free Kick Accuracy vs Vision**
"""

sns.set()
sns.lmplot(x='vision',y='free_kick_accuracy',data=player_with_attributes_df)
plt.title('Free Kick Accuracy vs Vision')
plt.xlabel('Vision')
plt.ylabel('Free Kick Accuracy');

"""**Sprint Speed vs Acceleration**"""

sns.set()
sns.lmplot(x='acceleration',y='sprint_speed',data=player_with_attributes_df)
plt.title('Sprint Speed vs Acceleration')
plt.xlabel('Acceleration')
plt.ylabel('Sprint Speed');

"""**Ball Control vs Finishing**"""

sns.set()
sns.lmplot(x='finishing',y='ball_control',data=player_with_attributes_df)
plt.title('Ball Control vs Finishing')
plt.xlabel('Finishing')
plt.ylabel('Ball Control');

"""## **11. What team traits lead to the most victories?**

Now, we need to join the team characteristics data with the results table, taking advantage of the cleaning operation that transformed the date column in the 'Team Attributes' table to correspond to the season column in the 'Match' table.
"""

# reset the index to turn 'season' and 'team_api_id' into columns
results_df = results_df.reset_index()

# convert the 'season' to match the 'year' in team_attributes_df_v2 (extract the first year of the season)
results_df['season'] = results_df['season'].str.split('/').str[0].astype(int)

# merge on 'team_api_id' and 'season'/'year'
team_atts_vs_results = pd.merge(left=team_attributes_df_v2, right=results_df, how='inner', left_on=['team_api_id', 'year'], right_on=['team_api_id', 'season'])

team_atts_vs_results.head(2)

"""The next collection of charts illustrates the relations between the numeric team qualities and the win ratio every season for each team. We should attempt to identify any significant link here so we can somehow know what are the most essential team attributes that should be considered to increase a team's win ratio."""

team_atts_vs_results.plot.scatter(x='buildUpPlaySpeed', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='buildUpPlayPassing', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='chanceCreationPassing', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='chanceCreationCrossing', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='chanceCreationShooting', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='defencePressure', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='defenceAggression', y='win_ratio_per_season')
team_atts_vs_results.plot.scatter(x='defenceTeamWidth', y='win_ratio_per_season');

"""The following group of charts shows the relations between the categorical team attributes and the win ratio per season for each team. We should try to find any strong relation here so we can somehow know what are the most important team attributes that should be considered to improve a team's win ratio."""

sns.catplot(x="buildUpPlaySpeedClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="buildUpPlayPassingClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="buildUpPlayPositioningClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="chanceCreationPassingClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="chanceCreationCrossingClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="chanceCreationShootingClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="chanceCreationPositioningClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="defencePressureClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="defenceAggressionClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="defenceTeamWidthClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results)
sns.catplot(x="defenceDefenderLineClass", y="win_ratio_per_season", kind='swarm', data=team_atts_vs_results);

"""## **12. Does various leagues have specific team characteristics?**"""

team_league_relation = match_df_v2[['league_id','home_team_api_id']]

team_league_relation

team_atts_vs_league_df = team_attributes_df.merge(team_league_relation, how='inner', left_on='team_api_id', right_on='home_team_api_id')

team_atts_vs_league_df

team_atts_vs_league_df.drop(['home_team_api_id'], axis=1, inplace=True)

# add the league name for visualization charts
team_atts_vs_league_df = team_atts_vs_league_df.merge(league_df, how='inner',left_on='league_id', right_on='id')

team_atts_vs_league_df.drop(['id_x','team_api_id','date','league_id','id_y','country_id','year'],axis=1, inplace=True)

team_atts_vs_league_df.info()

"""The next series of charts demonstrates the connections between leagues and team traits; this may assist us in knowing what the characteristics of each league are."""

for i, col in enumerate(team_atts_vs_league_df.columns):
    if ( type(team_atts_vs_league_df[col][0]) == type(team_atts_vs_league_df['buildUpPlaySpeed'][0]) ):
        plt.figure(i);
        chart = sns.boxplot(x=col, y="league_name", data=team_atts_vs_league_df);

"""# **Clustering Techniques**

## **Agglomerative Hierarchical Clustering (AHC)**
"""

import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.preprocessing import StandardScaler

player_with_attributes_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/player_att_merged.csv')

# calculate age
player_with_attributes_df['birthday'] = pd.to_datetime(player_with_attributes_df['birthday'])

def calculate_age(birthday):
    today = datetime.now()
    age = today.year - birthday.year - ((today.month, today.day) < (birthday.month, birthday.day))
    return age

player_with_attributes_df['age'] = player_with_attributes_df['birthday'].apply(calculate_age)

player_with_attributes_df['date'] = pd.to_datetime(player_with_attributes_df['date'])

player_with_attributes_df_2016 = player_with_attributes_df[player_with_attributes_df['date'].dt.year == 2016]

player_with_attributes_df_2016 = player_with_attributes_df_2016.sort_values(by='date', ascending=False)

player_with_attributes_df_2016 = player_with_attributes_df_2016.drop_duplicates(subset=['player_fifa_api_id', 'player_api_id'], keep='first')

"""### **Data Preprocessing**"""

# normalization
numerical_features = player_with_attributes_df_2016.select_dtypes(include=['float64', 'int64']).columns
scaler = StandardScaler()
player_with_attributes_df_2016[numerical_features] = scaler.fit_transform(player_with_attributes_df_2016[numerical_features])

# encode categorical variables
player_with_attributes_df_2016 = pd.get_dummies(player_with_attributes_df_2016, columns=['preferred_foot', 'attacking_work_rate', 'defensive_work_rate'])

"""### **Feature Selection**"""

from sklearn.feature_selection import SelectKBest, f_classif

# feature selection using SelectKBest
X = player_with_attributes_df_2016.drop(columns=['id', 'player_api_id', 'player_name', 'player_fifa_api_id', 'birthday', 'date', 'overall_rating'])
y = player_with_attributes_df_2016['overall_rating']

selector = SelectKBest(score_func=f_classif, k=10)
X_new = selector.fit_transform(X, y)
selected_features = X.columns[selector.get_support()]
print("Selected features using SelectKBest:", selected_features)

"""### **Principal Component Analysis (PCA)**"""

from sklearn.decomposition import PCA
import numpy as np

# PCA implementation
pca = PCA()
pca.fit(player_with_attributes_df_2016[selected_features])

# cumulative variance ratio
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_) * 100
print(cumulative_variance_ratio)

# plot cumulative explained variance
plt.figure(figsize=[10, 5])
plt.title('Cumulative Explained Variance explained by component')
plt.ylabel('Cumulative Explained variance (%)')
plt.xlabel('Principal components')
plt.plot(cumulative_variance_ratio)
plt.show()

# how many PCs explain 95% of the variance?
k = np.argmax(cumulative_variance_ratio > 95)
print("Number of components explaining 95% variance: " + str(k))

# PCA transformation
pca = PCA(n_components=k)
player_with_attributes_df_2016_pca = pca.fit_transform(player_with_attributes_df_2016[selected_features])

print('Principal Components:')
print(player_with_attributes_df_2016_pca)

"""### **Clustering**"""

from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score

# apply AHC with the best parameters
ahc = AgglomerativeClustering(n_clusters=2, linkage='ward')
clusters = ahc.fit_predict(player_with_attributes_df_2016_pca)

# visualize dendrogram
linked = linkage(player_with_attributes_df_2016_pca, 'ward')
plt.figure(figsize=(10, 7))
plt.title("Dendrogram for Agglomerative Hierarchical Clustering")
dendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.show()

# evaluation metrics
silhouette_avg = silhouette_score(player_with_attributes_df_2016_pca, clusters)
calinski_harabasz_avg = calinski_harabasz_score(player_with_attributes_df_2016_pca, clusters)
davies_bouldin_avg = davies_bouldin_score(player_with_attributes_df_2016_pca, clusters)

print(f'Silhouette Score: {silhouette_avg}')
print(f'Calinski-Harabasz Index: {calinski_harabasz_avg}')
print(f'Davies-Bouldin Index: {davies_bouldin_avg}')

"""### **Validation**"""

from sklearn.model_selection import cross_val_score, train_test_split
from sklearn.metrics import make_scorer

# cross-validation
silhouette_scorer = make_scorer(silhouette_score, greater_is_better=True)
scores = cross_val_score(ahc, player_with_attributes_df_2016_pca, cv=5, scoring=silhouette_scorer)
print(f'Cross-Validation Silhouette Scores: {scores}')
print(f'Mean Cross-Validation Silhouette Score: {scores.mean()}')

# holdout Validation
X_train, X_test = train_test_split(player_with_attributes_df_2016_pca, test_size=0.2, random_state=42)
ahc.fit(X_train)
clusters_test = ahc.fit_predict(X_test)
silhouette_avg_test = silhouette_score(X_test, clusters_test)
print(f'Test Silhouette Score: {silhouette_avg_test}')

"""### **Visualization**"""

plt.figure(figsize=(10,6))

# plot the data points with color corresponding to their cluster label
scatter = plt.scatter(player_with_attributes_df_2016_pca[:, 0], player_with_attributes_df_2016_pca[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.6, edgecolor='k')

# create a custom legend for clusters
unique_labels = np.unique(clusters)
handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=plt.cm.viridis(i / (len(unique_labels) - 1)), markersize=10, label=f'Cluster {i}') for i in unique_labels]

# add custom legend handles to the plot
plt.legend(handles=handles)

# add titles and labels
plt.title('Agglomerative Clustering')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')

plt.show()

"""## **Mean Shift**"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import MeanShift, estimate_bandwidth
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score
from datetime import datetime
from sklearn.decomposition import PCA

player_with_attributes_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/player_att_merged.csv')\

player_with_attributes_df['date'] = pd.to_datetime(player_with_attributes_df['date'])

player_with_attributes_2016_df = player_with_attributes_df[player_with_attributes_df['date'].dt.year == 2016]

"""### **Sampling**"""

physical_attributes = player_with_attributes_2016_df[['volleys','crossing','balance','curve','dribbling']]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(physical_attributes)

print(physical_attributes.columns)

"""### **Scaling**"""

# Perform Mean-Shift Clustering
mean_shift = MeanShift()
mean_shift.fit(X_scaled)

# Get cluster labels
labels = mean_shift.labels_
player_with_attributes_2016_df['Cluster'] = labels

"""**PCA**"""

from sklearn.decomposition import PCA

pca = PCA()
pca.fit(X_scaled)

# Getting the cumulative variance
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)*100
print(cumulative_variance_ratio)

k = np.argmax(cumulative_variance_ratio>95)
print("Number of components explaining 95% variance: "+ str(k))

plt.figure(figsize=[10,5])
plt.title('Cumulative Explained Variance explained by component')
plt.ylabel('Cumulative Explained variance (%)')
plt.xlabel('Principal components')
plt.axvline(x=k, color="k", linestyle="--")
plt.axhline(y=95, color="r", linestyle="--")
ax = plt.plot(cumulative_variance_ratio)

len(pca.components_)

pca = PCA(n_components=2)
data_pca = pca.fit_transform(X_scaled)

print('Principal Components:')
print(data_pca)

import plotly.express as px
inertia_meanshift = []

# Define a range of bandwidth values to try
bandwidth_range = np.linspace(0.1, 2.0, 10)

for bandwidth in bandwidth_range:
    meanshift = MeanShift(bandwidth=bandwidth)
    meanshift.fit(data_pca)

    # Get the cluster centers and labels
    labels = meanshift.labels_
    cluster_centers = meanshift.cluster_centers_

    distances = np.linalg.norm(data_pca - cluster_centers[labels], axis=1)
    inertia_meanshift.append(np.sum(distances**2))

# Plot the result
px.line(x=bandwidth_range, y=inertia_meanshift,
        title="Elbow graph for MeanShift Clustering",
        labels={"x": "Bandwidth", "y": "Inertia-like"})

"""### **Parameter Tuning**"""

from sklearn.model_selection import KFold
from sklearn.neighbors import NearestNeighbors

bandwidths = [0.8, 1.0, 1.2]
average_silhouette_scores = []

# KFold Cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Cross-validation loop for different bandwidths
for bandwidth in bandwidths:
    silhouette_scores = []
    all_cluster_labels = []
    all_test_indices = []

    print(f"\nPerforming cross-validation for bandwidth: {bandwidth}")

    for train_index, test_index in kf.split(data_pca):
        # Split the data into training and testing sets using the indices
        X_train, X_test = data_pca[train_index], data_pca[test_index]

        # Initialize and fit MeanShift model with specified bandwidth
        meanshift = MeanShift(bandwidth=bandwidth)

        # Fit and predict cluster labels on test set
        cluster_labels = meanshift.fit_predict(X_test)
        all_cluster_labels.append(cluster_labels)
        all_test_indices.append(test_index)

        if len(np.unique(cluster_labels)) > 1:
            score = silhouette_score(X_test, cluster_labels)
            silhouette_scores.append(score)
        else:
            silhouette_scores.append(0)

    # Calculate average silhouette score
    avg_score = np.mean(silhouette_scores)
    average_silhouette_scores.append(avg_score)

    # Print silhouette scores and average for this bandwidth
    print(f"Silhouette scores for bandwidth {bandwidth}: {silhouette_scores}")
    print(f"Average silhouette score for bandwidth {bandwidth}: {avg_score}")

    # Visualize clustering results for this bandwidth (PCA plot)
    plt.figure(figsize=(8, 6))
    for fold, (test_index, cluster_labels) in enumerate(zip(all_test_indices, all_cluster_labels)):
        plt.scatter(data_pca[test_index, 0], data_pca[test_index, 1], c=cluster_labels, cmap='viridis', label=f'Fold {fold+1}', alpha=0.5)

    plt.title(f'MeanShift Clustering Visualization (Bandwidth = {bandwidth})')
    plt.xlabel('PCA Component 1')
    plt.ylabel('PCA Component 2')
    plt.colorbar(label='Cluster')
    plt.legend()
    plt.show()

# Plotting a Bar Graph to compare MeanShift models with different bandwidths
plt.figure(figsize=(8, 6))
plt.bar([str(bw) for bw in bandwidths], average_silhouette_scores)
plt.xlabel('Bandwidth')
plt.ylabel('Average Silhouette Score')
plt.title('Comparison of MeanShift Models with Different Bandwidths')

plt.show()

from sklearn.neighbors import NearestNeighbors

# Estimate the bandwidth using NearestNeighbors
neigh = NearestNeighbors(n_neighbors=2)
neigh.fit(X_scaled)
distances, _ = neigh.kneighbors(X_scaled)
bandwidth = np.mean(distances)

# Test a range of bandwidth values around the estimated bandwidth
bandwidth_values = np.linspace(0.1, bandwidth * 2, 10)  # Modify range as necessary
best_bandwidth = None
best_score = -1

for bw in bandwidth_values:
    mean_shift = MeanShift(bandwidth=bw)
    labels = mean_shift.fit_predict(X_scaled)

    n_clusters = len(np.unique(labels))
    if n_clusters > 1:
        score = silhouette_score(X_scaled, labels)
        print(f'Bandwidth: {bw}, Silhouette Score: {score}')

        if score > best_score:
            best_score = score
            best_bandwidth = bw

if best_bandwidth:
    print(f'Best Bandwidth: {best_bandwidth}')
    print(f'Best Silhouette Score: {best_score}')
else:
    print("All tested bandwidths result in a single cluster. Try reducing the range.")

"""### **Perform Mean-Shift**"""

# Visualization of Clusters vs. PCA Components using MeanShift on subset
plt.figure(figsize=(12, 8))

# Ensure you only plot the subset of the data used in clustering
scatter = plt.scatter(data_pca[test_index, 0], data_pca[test_index, 1], c=meanshift.labels_, cmap='viridis', marker='o', alpha=0.5)

# Add title and labels
plt.title('PCA of Data with MeanShift Clustering Results')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, label='Cluster Label')

plt.show()

"""### **Evaluation Metrics**"""

# Perform Mean-Shift clustering
mean_shift = MeanShift(bandwidth=best_bandwidth)  # Use the best_bandwidth found in parameter tuning
labels = mean_shift.fit_predict(X_scaled)

# Reduce dimensionality for visualization
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

# Evaluation metrics
silhouette_avg = silhouette_score(X_scaled, labels)
calinski_harabasz = calinski_harabasz_score(X_scaled, labels)
davies_bouldin = davies_bouldin_score(X_scaled, labels)

print(f'Silhouette Score: {silhouette_avg}')
print(f'Calinski-Harabasz Index: {calinski_harabasz}')
print(f'Davies-Bouldin Index: {davies_bouldin}')

# Plotting
plt.figure(figsize=(12, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', alpha=0.5)
plt.title('Mean-Shift Clustering')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.colorbar(scatter, label='Cluster Label')
plt.show()

"""## **Spectral Clustering**"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, normalize
from sklearn.cluster import SpectralClustering
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

#Take data from 2014-2016
data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/match_merged.csv')
data_selected_years = data[data['match_year'].isin([2016, 2015, 2014])]

#Encode 'winning_team' (home win = 1, draw = 0, away win = -1)
data_selected_years['winning_team'] = data_selected_years['winning_team'].map({'Home': 1, 'Draw': 0, 'Away': -1})

print(data_selected_years.columns)

features1 = data_selected_years[['B365H', 'B365D', 'B365A','BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA', 'VCH', 'VCD', 'VCA', 'winning_team']]

"""### **Remove Outliers**"""

from sklearn.ensemble import IsolationForest
# Step 1: Initialize the Isolation Forest
iso_forest = IsolationForest(contamination=0.05, random_state=42)

# Step 2: Fit the model to your data
iso_forest.fit(features1)

# Step 3: Predict which points are outliers (-1 for outliers, 1 for inliers)
outliers = iso_forest.predict(features1)

# Step 4: Create a new DataFrame excluding the outliers
data_no_outliers = features1[outliers == 1]

# Step 5: Print the results
print(f"Original dataset size: {features1.shape}")
print(f"Dataset size after outlier removal: {data_no_outliers.shape}")

features2 = ['B365H', 'B365D', 'B365A','BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'LBH', 'LBD', 'LBA', 'WHH', 'WHD', 'WHA', 'VCH', 'VCD', 'VCA']

"""### **Sampling**"""

X = data_no_outliers[features2]
y = data_no_outliers['winning_team']

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

"""### **Scaling**"""

scaler = StandardScaler()
data_scaled = scaler.fit_transform(X_resampled)

"""### **PCA**"""

from sklearn.decomposition import PCA
import numpy as np
# Plotting the cumulative variance ratio can help decide the number of components
import matplotlib.pyplot as plt

pca = PCA()
pca.fit(data_scaled)

# Getting the cumulative variance
cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)*100
print(cumulative_variance_ratio)

# How many PCs explain 95% of the variance?
k = np.argmax(cumulative_variance_ratio>95)
print("Number of components explaining 95% variance: "+ str(k))
#print("\n")

plt.figure(figsize=[10,5])
plt.title('Cumulative Explained Variance explained by component')
plt.ylabel('Cumulative Explained variance (%)')
plt.xlabel('Principal components')
plt.axvline(x=k, color="k", linestyle="--")
plt.axhline(y=95, color="r", linestyle="--")
ax = plt.plot(cumulative_variance_ratio)

len(pca.components_)

pca = PCA(n_components=1)
data_pca = pca.fit_transform(data_scaled)

print('Principal Components:')
print(data_pca)

import plotly.express as px
from sklearn.cluster import SpectralClustering , KMeans
inertia = []
for i in range(1,10):
    cluster = KMeans(n_clusters=i)
    cluster.fit(data_pca)
    inertia.append(cluster.inertia_)
px.line(inertia, title="Elbow graph for Spectral Clustering",labels={"index":"Clusters","value":"Inertia"})

"""### **Perform Spectral Clustering**"""

# Perform Spectral Clustering
spectral_clustering = SpectralClustering(n_clusters=3, affinity='rbf', gamma=10, random_state=42)
clusters = spectral_clustering.fit_predict(data_pca)

plt.figure(figsize=(10, 6))
plt.scatter(range(len(test_index)), data_pca[test_index, 0], c=clusters[test_index], cmap='viridis') # Use clusters[test_index] to select the corresponding cluster labels for the test set
plt.title('Spectral Clustering based on Betting Odds')
plt.xlabel('Data Point Index')
plt.ylabel('PCA Component')
plt.colorbar(label='Cluster')
plt.show()

"""### **Evaluation Metrics**"""

from sklearn.metrics import davies_bouldin_score

# Compute Davies-Bouldin Index using PCA-transformed data
db_score = davies_bouldin_score(data_pca, clusters)
print(f'Davies-Bouldin Index (on PCA-transformed data): {db_score}')

from sklearn.metrics import calinski_harabasz_score

# Compute Calinski-Harabasz Index
ch_score = calinski_harabasz_score(data_pca, clusters)
print(f'Calinski-Harabasz Index: {ch_score}')

"""### **Validation**"""

y_resampled_df = y_resampled.to_frame()
print(y_resampled_df.columns)

# Add cluster assignments as a new column to the DataFrame.
X_resampled['Spectral_Cluster'] = clusters

# Analyze clusters with respect to prediction quality
for cluster_id in range(3):
    cluster_data = y_resampled_df[X_resampled['Spectral_Cluster'] == cluster_id]
    quality_distribution = cluster_data['winning_team'].value_counts()
    print(f'Cluster {cluster_id} Prediction Quality Distribution:')
    print(quality_distribution)
    print()

# Visualization of Clusters vs. Actual Outcomes
plt.figure(figsize=(12, 8))
scatter = plt.scatter(range(len(test_index)), data_pca[test_index, 0], c=y_resampled[test_index], cmap='viridis', marker='o', alpha=0.5, label='Actual Outcomes')
plt.title('PCA of Data with Actual Outcomes')
plt.xlabel('Data Point Index')
plt.ylabel('PCA Component')
plt.colorbar(scatter, label='Actual Outcome')
plt.show()

# Visualization of Clusters
plt.figure(figsize=(12, 8))
scatter = plt.scatter(range(len(test_index)), data_pca[test_index, 0], c=clusters[test_index], cmap='viridis', marker='o', alpha=0.5, label='Clusters')
plt.title('PCA of Data with Clustering Results')
plt.xlabel('Data Point Index')
plt.ylabel('PCA Component')
plt.colorbar(scatter, label='Cluster')
plt.show()

"""## **Expectation–Maximization Algorithm (EM)**"""

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from sklearn.model_selection import KFold, RandomizedSearchCV
from scipy.stats import uniform
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import sqlite3
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

from google.colab import drive
drive.mount('/content/drive/')

team_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/team_att_merged.csv')

# Define the features
features = ['buildUpPlaySpeed', 'chanceCreationPassing',
            'chanceCreationCrossing', 'chanceCreationShooting',
            'defencePressure', 'defenceAggression', 'defenceTeamWidth']

# Define X (features) and y (target)
X_team = team_data[features]
y = team_data['year']

# --- Step 2: Feature Selection using RFE ---
model = LogisticRegression()
rfe = RFE(model, n_features_to_select=5)
X_team_selected = rfe.fit_transform(X_team, y)

# Print selected features and ranking
selected_features = [features[i] for i in range(len(features)) if rfe.support_[i]]
print("Selected Features:", selected_features)

"""### **Scaling**"""

# Preprocess the data by scaling it
scaler = StandardScaler()
X_team_scaled = scaler.fit_transform(X_team_selected)      # Team strategy data

"""### **Hyperparameter Tuning**"""

# Define a function to perform hyperparameter tuning with a fixed number of components
def tune_hyperparameters(X, n_components, title):
    param_dist = {
        'covariance_type': ['full', 'tied', 'diag', 'spherical'],
        'max_iter': [100, 200, 300]
    }

    # Initialize GaussianMixture with fixed n_components
    gmm = GaussianMixture(n_components=n_components, random_state=42)

    # Perform random search for hyperparameter tuning
    random_search = RandomizedSearchCV(gmm, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, random_state=42)
    random_search.fit(X)

    best_gmm = random_search.best_estimator_
    print(f"Best Parameters for {title} Clustering:", random_search.best_params_)
    return best_gmm

# Perform hyperparameter tuning with n_components fixed at 3
n_components_team = 3

print("Tuning Hyperparameters for Team Strategy Clustering")
best_gmm_team = tune_hyperparameters(X_team_scaled, n_components_team, "Team Strategy")

"""### **Principal Component Analysis (PCA)**"""

def plot_pca_variance(X, title):
    # Initialize PCA
    pca = PCA()
    pca.fit(X)

    # Calculate cumulative explained variance ratio
    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_) * 100
    print(f"Cumulative Explained Variance Ratio for {title}:")
    print(cumulative_variance_ratio)

    # Print number of components explaining 95% variance
    k = np.argmax(cumulative_variance_ratio > 95) + 1  # +1 to get the actual number of components
    print(f"Number of components explaining 95% variance for {title}: {k}")

    # Plot cumulative explained variance
    plt.figure(figsize=[10,5])
    plt.title(f'Cumulative Explained Variance for {title}')
    plt.ylabel('Cumulative Explained Variance (%)')
    plt.xlabel('Principal Components')
    plt.axvline(x=k, color="k", linestyle="--", label=f'95% Explained Variance')
    plt.axhline(y=95, color="r", linestyle="--", label='95% Threshold')
    plt.plot(cumulative_variance_ratio, label='Cumulative Explained Variance')
    plt.legend()
    plt.show()

# Plot for Team Strategy data
plot_pca_variance(X_team_scaled, "Team Strategy")

"""### **Clustering & Evaluation Metrics**"""

# Define a function to perform EM clustering and evaluate metrics
def perform_em_clustering(X, gmm, title):
    labels = gmm.predict(X)

    silhouette = silhouette_score(X, labels)
    davies_bouldin = davies_bouldin_score(X, labels)
    calinski_harabasz = calinski_harabasz_score(X, labels)
    aic = gmm.aic(X)
    bic = gmm.bic(X)

    print(f"{title} Clustering - Number of Clusters: {gmm.n_components}")
    print(f"Silhouette Score: {silhouette}")
    print(f"Davies-Bouldin Index: {davies_bouldin}")
    print(f"Calinski-Harabasz Index: {calinski_harabasz}")
    print(f"AIC: {aic}")
    print(f"BIC: {bic}")
    print("-" * 50)

    return labels

# Perform EM clustering and evaluate for Team Strategy
print("Evaluating Team Strategy Clustering")
team_labels = perform_em_clustering(X_team_scaled, best_gmm_team, "Team Strategy")

"""### **Validation**"""

# Define a function for cross-validation of EM clustering with additional metrics
def cross_validate_em(X, n_components, covariance_type, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    silhouette_scores = []
    davies_bouldin_scores = []
    calinski_harabasz_scores = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]

        gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)
        gmm.fit(X_train)

        labels_test = gmm.predict(X_test)

        silhouette = silhouette_score(X_test, labels_test)
        davies_bouldin = davies_bouldin_score(X_test, labels_test)
        calinski_harabasz = calinski_harabasz_score(X_test, labels_test)

        silhouette_scores.append(silhouette)
        davies_bouldin_scores.append(davies_bouldin)
        calinski_harabasz_scores.append(calinski_harabasz)

    avg_silhouette = np.mean(silhouette_scores)
    avg_davies_bouldin = np.mean(davies_bouldin_scores)
    avg_calinski_harabasz = np.mean(calinski_harabasz_scores)

    print(f"Cross-Validated Silhouette Score: {avg_silhouette}")
    print(f"Cross-Validated Davies-Bouldin Index: {avg_davies_bouldin}")
    print(f"Cross-Validated Calinski-Harabasz Index: {avg_calinski_harabasz}")

    return avg_silhouette, avg_davies_bouldin, avg_calinski_harabasz

# Validate Team Strategy Clustering
print("\nValidating Team Strategy Clustering:")
cross_validate_em(X_team_scaled, n_components_team, best_gmm_team.covariance_type)

"""### **Visualization**"""

def visualize_clusters_2d(X, labels, title):
    # Reduce dimensions to 2D for visualization
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # Create a scatter plot of the clusters
    plt.figure(figsize=(10, 7))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', s=50, alpha=0.7)

    plt.colorbar(scatter, label='Cluster')
    plt.title(f"{title} Clustering Visualization (PCA)")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.show()

# Visualize Team Strategy Clustering
visualize_clusters_2d(X_team_scaled, team_labels, "Team Strategy")

def plot_pairwise_clusters(X, labels, title):
    # Convert labels to a DataFrame for easier plotting
    df = pd.DataFrame(X, columns=[f"Feature {i+1}" for i in range(X.shape[1])])
    df['Cluster'] = labels

    # Plot pairwise features with hue as the cluster labels
    sns.pairplot(df, hue='Cluster', palette='viridis', markers='o', diag_kind='kde')
    plt.suptitle(f"{title} Pairwise Feature Distribution", y=1.02)
    plt.show()

# Plot Pairwise Features for Team Strategy Clustering
plot_pairwise_clusters(X_team_scaled, team_labels, "Team Strategy")

def plot_cluster_centroids(gmm, X, title):
    # Get means of Gaussian components
    means = gmm.means_

    # Reduce dimensionality for visualization
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    means_pca = pca.transform(means)

    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=team_labels, palette='viridis', marker='o', s=50, alpha=0.7)
    plt.scatter(means_pca[:, 0], means_pca[:, 1], s=200, c='red', marker='X', label='Centroids')
    plt.title(f"{title} Clustering with Centroids (PCA)")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.legend()
    plt.show()

# Plot Cluster Centroids for Team Strategy Clustering
plot_cluster_centroids(best_gmm_team, X_team_scaled, "Team Strategy")

from sklearn.manifold import TSNE

def visualize_clusters_tsne(X, labels, title):
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette='viridis', marker='o', s=50, alpha=0.7)
    plt.title(f"{title} Clustering Visualization (t-SNE)")
    plt.xlabel("t-SNE Dimension 1")
    plt.ylabel("t-SNE Dimension 2")
    plt.legend(title='Cluster')
    plt.show()

# Visualize with t-SNE
visualize_clusters_tsne(X_team_scaled, team_labels, "Team Strategy")

!pip install umap-learn

import umap

def visualize_clusters_umap(X, labels, title):
    umap_model = umap.UMAP(n_components=2, random_state=42)
    X_umap = umap_model.fit_transform(X)

    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=X_umap[:, 0], y=X_umap[:, 1], hue=labels, palette='viridis', marker='o', s=50, alpha=0.7)
    plt.title(f"{title} Clustering Visualization (UMAP)")
    plt.xlabel("UMAP Dimension 1")
    plt.ylabel("UMAP Dimension 2")
    plt.legend(title='Cluster')
    plt.show()

# Visualize with UMAP
visualize_clusters_umap(X_team_scaled, team_labels, "Team Strategy")

"""# **Streamlit URL**

## **Agglomerative Hierarchical Clustering (AHC)**

https://europeansoccerahc.streamlit.app/

## **Mean Shift**

https://europeansoccermeanshift.streamlit.app/

## **Spectral Clustering**

https://testing-daccdzlxrjx2n3qcvwnxqz.streamlit.app/

## **Expectation–Maximization Algorithm (EM)**
"""