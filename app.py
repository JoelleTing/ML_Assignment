# -*- coding: utf-8 -*-
"""Coding-main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m_f6k7X2Pan9cXEcLN3p-KWUuHJ3wNCM

# **Introduction**
  Football is an important part of our society. It not only provides entertainment, but also maintains the welfare of football players and generates significant revenue during FIFA and other competitions. These activities attract millions of spectators to stadiums and television sets. Numerous supporters place wagers on their preferred teams. Many of them lose their money, while a few of them succeed. We will examine the [European Soccer Database](https://www.kaggle.com/datasets/hugomathien/soccer), which is provided by Hugo Mathien, in order to predict and enhance the likelihood that the game tally will be in their favour.

  The dataset encompasses information on football players and teams from numerous European countries, as well as match results, from 2008 to 2016. The data is recorded in a SQLite database, which comprises seven tables: “Country”, “League”, “Match”, “Player”, “Player_Attributes”, “Team”, and “Team_Attributes”. The “Country” table contains 11 rows and 2 columns and has a relationship with the “League”, which has 11 rows and 3 columns, and the “Team”, which has 299 rows and 5 columns. The “Team_Attributes” table includes 1458 rows and 25 columns, each of which is related to a specific team. “Match” has 25979 rows and 115 columns that relate to the “Team” table. The “Player” table has 11060 rows and 7 columns and has a relationship with “Player_Attributes” with 183978 rows and 42 columns.

# **Literature Review**
"""

# importing the necessary libraries
# data processing, CSV file I/O (e.g. pd.read_csv)
import pandas as pd
import sqlite3
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime<svg width="38" height="38" viewBox="0 0 38 38" fill="none" xmlns="http://www.w3.org/2000/svg">
<g id="Logo">
<circle id="Ellipse 8" cx="19" cy="19" r="18" fill="#FBFFFF"/>
<path id="Vector" d="M19.2984 29.6439C13.2569 29.7033 8.38811 24.8048 8.44748 18.793C8.35842 12.8109 13.2569 7.91244 19.239 7.97182C22.4156 7.94213 24.8945 8.96636 27.0469 11L24.2711 13.7758C22.9945 12.3805 21.2429 11.757 19.2241 11.757C15.2311 11.6976 12.5444 14.8445 12.6335 18.7782C12.5147 22.6821 15.2608 25.8884 19.2538 25.7993C21.3765 25.829 23.039 25.161 24.4195 23.736L27.1953 26.5118C25.0429 28.5306 22.3859 29.6736 19.2984 29.6439Z" fill="#0B163E"/>
<path id="Vector_2" d="M33.8747 30.8311C40.2279 22.7264 39.1591 11.2669 31.2473 4.49808C22.7121 -2.90902 9.20416 -0.919936 3.08848 8.63952C-4.31862 20.0099 2.30175 35.1804 15.4534 37.6296C22.1926 39.0546 29.6442 36.264 33.8747 30.8311ZM31.2176 27.6545C28.4715 31.5288 24.048 33.978 19.0011 33.978C8.20962 34.0374 0.83221 22.5482 5.35959 12.781C6.50257 10.3317 8.269 8.11999 10.5846 6.65044C16.73 2.2715 25.6809 3.74104 30.327 9.3075C34.8099 14.5325 35.1661 22.2068 31.2176 27.6545Z" fill="#3F9AF5"/>
</g>
</svg>

from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, cut_tree
from sklearn.datasets import make_blobs
from sklearn.cluster import KMeans
from sklearn.preprocessing import normalize, StandardScaler
import scipy.cluster.hierarchy as shc
from sklearn.cluster import AgglomerativeClustering
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from scipy.spatial.distance import squareform, pdist

from google.colab import drive
drive.mount('/content/drive/')

"""## **Expectation–Maximization Algorithm (EM)**"""

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.mixture import GaussianMixture
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from sklearn.model_selection import KFold, RandomizedSearchCV
from scipy.stats import uniform
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
import sqlite3
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

from google.colab import drive
drive.mount('/content/drive/')

team_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/European Soccer/team_att_merged.csv')

# Define the features
features = ['buildUpPlaySpeed', 'chanceCreationPassing',
            'chanceCreationCrossing', 'chanceCreationShooting',
            'defencePressure', 'defenceAggression', 'defenceTeamWidth']

# Define X (features) and y (target)
X_team = team_data[features]
y = team_data['year']

# --- Step 2: Feature Selection using RFE ---
model = LogisticRegression()
rfe = RFE(model, n_features_to_select=5)
X_team_selected = rfe.fit_transform(X_team, y)

# Print selected features and ranking
selected_features = [features[i] for i in range(len(features)) if rfe.support_[i]]
print("Selected Features:", selected_features)

"""### **Scaling**"""

# Preprocess the data by scaling it
scaler = StandardScaler()
X_team_scaled = scaler.fit_transform(X_team_selected)      # Team strategy data

"""### **Hyperparameter Tuning**"""

# Define a function to perform hyperparameter tuning with a fixed number of components
def tune_hyperparameters(X, n_components, title):
    param_dist = {
        'covariance_type': ['full', 'tied', 'diag', 'spherical'],
        'max_iter': [100, 200, 300]
    }

    # Initialize GaussianMixture with fixed n_components
    gmm = GaussianMixture(n_components=n_components, random_state=42)

    # Perform random search for hyperparameter tuning
    random_search = RandomizedSearchCV(gmm, param_distributions=param_dist, n_iter=10, cv=3, verbose=1, random_state=42)
    random_search.fit(X)

    best_gmm = random_search.best_estimator_
    print(f"Best Parameters for {title} Clustering:", random_search.best_params_)
    return best_gmm

# Perform hyperparameter tuning with n_components fixed at 3
n_components_team = 3

print("Tuning Hyperparameters for Team Strategy Clustering")
best_gmm_team = tune_hyperparameters(X_team_scaled, n_components_team, "Team Strategy")

"""### **Principal Component Analysis (PCA)**"""

def plot_pca_variance(X, title):
    # Initialize PCA
    pca = PCA()
    pca.fit(X)

    # Calculate cumulative explained variance ratio
    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_) * 100
    print(f"Cumulative Explained Variance Ratio for {title}:")
    print(cumulative_variance_ratio)

    # Print number of components explaining 95% variance
    k = np.argmax(cumulative_variance_ratio > 95) + 1  # +1 to get the actual number of components
    print(f"Number of components explaining 95% variance for {title}: {k}")

    # Plot cumulative explained variance
    plt.figure(figsize=[10,5])
    plt.title(f'Cumulative Explained Variance for {title}')
    plt.ylabel('Cumulative Explained Variance (%)')
    plt.xlabel('Principal Components')
    plt.axvline(x=k, color="k", linestyle="--", label=f'95% Explained Variance')
    plt.axhline(y=95, color="r", linestyle="--", label='95% Threshold')
    plt.plot(cumulative_variance_ratio, label='Cumulative Explained Variance')
    plt.legend()
    plt.show()

# Plot for Team Strategy data
plot_pca_variance(X_team_scaled, "Team Strategy")

"""### **Clustering & Evaluation Metrics**"""

# Define a function to perform EM clustering and evaluate metrics
def perform_em_clustering(X, gmm, title):
    labels = gmm.predict(X)

    silhouette = silhouette_score(X, labels)
    davies_bouldin = davies_bouldin_score(X, labels)
    calinski_harabasz = calinski_harabasz_score(X, labels)
    aic = gmm.aic(X)
    bic = gmm.bic(X)

    print(f"{title} Clustering - Number of Clusters: {gmm.n_components}")
    print(f"Silhouette Score: {silhouette}")
    print(f"Davies-Bouldin Index: {davies_bouldin}")
    print(f"Calinski-Harabasz Index: {calinski_harabasz}")
    print(f"AIC: {aic}")
    print(f"BIC: {bic}")
    print("-" * 50)

    return labels

# Perform EM clustering and evaluate for Team Strategy
print("Evaluating Team Strategy Clustering")
team_labels = perform_em_clustering(X_team_scaled, best_gmm_team, "Team Strategy")

"""### **Validation**"""

# Define a function for cross-validation of EM clustering with additional metrics
def cross_validate_em(X, n_components, covariance_type, n_splits=5):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    silhouette_scores = []
    davies_bouldin_scores = []
    calinski_harabasz_scores = []

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]

        gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type, random_state=42)
        gmm.fit(X_train)

        labels_test = gmm.predict(X_test)

        silhouette = silhouette_score(X_test, labels_test)
        davies_bouldin = davies_bouldin_score(X_test, labels_test)
        calinski_harabasz = calinski_harabasz_score(X_test, labels_test)

        silhouette_scores.append(silhouette)
        davies_bouldin_scores.append(davies_bouldin)
        calinski_harabasz_scores.append(calinski_harabasz)

    avg_silhouette = np.mean(silhouette_scores)
    avg_davies_bouldin = np.mean(davies_bouldin_scores)
    avg_calinski_harabasz = np.mean(calinski_harabasz_scores)

    print(f"Cross-Validated Silhouette Score: {avg_silhouette}")
    print(f"Cross-Validated Davies-Bouldin Index: {avg_davies_bouldin}")
    print(f"Cross-Validated Calinski-Harabasz Index: {avg_calinski_harabasz}")

    return avg_silhouette, avg_davies_bouldin, avg_calinski_harabasz

# Validate Team Strategy Clustering
print("\nValidating Team Strategy Clustering:")
cross_validate_em(X_team_scaled, n_components_team, best_gmm_team.covariance_type)

"""### **Visualization**"""

def visualize_clusters_2d(X, labels, title):
    # Reduce dimensions to 2D for visualization
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)

    # Create a scatter plot of the clusters
    plt.figure(figsize=(10, 7))
    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', marker='o', s=50, alpha=0.7)

    plt.colorbar(scatter, label='Cluster')
    plt.title(f"{title} Clustering Visualization (PCA)")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.show()

# Visualize Team Strategy Clustering
visualize_clusters_2d(X_team_scaled, team_labels, "Team Strategy")

def plot_pairwise_clusters(X, labels, title):
    # Convert labels to a DataFrame for easier plotting
    df = pd.DataFrame(X, columns=[f"Feature {i+1}" for i in range(X.shape[1])])
    df['Cluster'] = labels

    # Plot pairwise features with hue as the cluster labels
    sns.pairplot(df, hue='Cluster', palette='viridis', markers='o', diag_kind='kde')
    plt.suptitle(f"{title} Pairwise Feature Distribution", y=1.02)
    plt.show()

# Plot Pairwise Features for Team Strategy Clustering
plot_pairwise_clusters(X_team_scaled, team_labels, "Team Strategy")

def plot_cluster_centroids(gmm, X, title):
    # Get means of Gaussian components
    means = gmm.means_

    # Reduce dimensionality for visualization
    pca = PCA(n_components=2)
    X_pca = pca.fit_transform(X)
    means_pca = pca.transform(means)

    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=team_labels, palette='viridis', marker='o', s=50, alpha=0.7)
    plt.scatter(means_pca[:, 0], means_pca[:, 1], s=200, c='red', marker='X', label='Centroids')
    plt.title(f"{title} Clustering with Centroids (PCA)")
    plt.xlabel("Principal Component 1")
    plt.ylabel("Principal Component 2")
    plt.legend()
    plt.show()

# Plot Cluster Centroids for Team Strategy Clustering
plot_cluster_centroids(best_gmm_team, X_team_scaled, "Team Strategy")

from sklearn.manifold import TSNE

def visualize_clusters_tsne(X, labels, title):
    tsne = TSNE(n_components=2, random_state=42)
    X_tsne = tsne.fit_transform(X)

    plt.figure(figsize=(10, 7))
    sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=labels, palette='viridis', marker='o', s=50, alpha=0.7)
    plt.title(f"{title} Clustering Visualization (t-SNE)")
    plt.xlabel("t-SNE Dimension 1")
    plt.ylabel("t-SNE Dimension 2")
    plt.legend(title='Cluster')
    plt.show()

# Visualize with t-SNE
visualize_clusters_tsne(X_team_scaled, team_labels, "Team Strategy")
